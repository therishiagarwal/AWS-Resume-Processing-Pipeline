# ğŸ“„ Resume Processing Pipeline using AWS Serverless Services

This project is a **serverless resume parsing pipeline** on AWS that processes PDF resumes, extracts structured data, stores it, and enables visual insights using AWS QuickSight. The system is fully automated using **Lambda**, **S3**, **DynamoDB**, **Glue**, **Athena**, and **QuickSight**.

---

## ğŸ Getting Started

### Prerequisites
- AWS Account with permissions for Lambda, S3, DynamoDB, Glue, Athena, QuickSight
- Docker & AWS CLI installed and configured

### Quick Start
- bash
    ```bash
    # Clone repo
    git clone https://github.com/your-username/resume-pipeline.git
    cd resume-pipeline

    # Build Docker and deploy Lambda
    ./deployment/deploy_lambda.sh

    # Deploy CloudFormation stack
    aws cloudformation deploy \
    --template-file cloudformation_templates/deployment_template.yaml \
    --stack-name ResumePipeline

## ğŸš€ Features

- ğŸ“¤ Upload PDF resumes via API Gateway
- ğŸª„ Extract text/data using Lambda functions (Dockerized for custom parsing)
- â˜ï¸ Store raw PDFs in S3 and extracted data in DynamoDB
- ğŸ” Automatic Glue crawler + ETL transformation
- ğŸ” Query structured data with Athena
- ğŸ“Š Create interactive dashboards with QuickSight
- ğŸ”” Logs & monitoring via CloudWatch

---

## ğŸ“Œ Architecture Overview

![Architecture](architecture/architecture_diagram.png)

- **User** uploads resume via REST API Gateway
- **Lambda1** stores PDF in **S3 (raw)**
- **S3 trigger** invokes **Lambda2** to extract and insert into **DynamoDB**
- **AWS Glue** crawls and transforms data into **S3 (transformed)**
- **Athena** runs queries on transformed data
- **QuickSight** dashboard visualizes insights

---

## ğŸ› ï¸ Tech Stack

| Service      | Purpose                                      |
|--------------|----------------------------------------------|
| API Gateway  | Resume Upload Endpoint                       |
| Lambda       | PDF Upload Handler & Data Extractor          |
| S3           | Store raw and transformed data               |
| DynamoDB     | Store extracted structured resume data       |
| AWS Glue     | Crawl + Transform data                       |
| Athena       | Query transformed JSON data                  |
| QuickSight   | Visual dashboards from Athena datasets       |
| CloudWatch   | Monitoring and Logs                          |

---

## ğŸ“‚ Project Structure

- Here is the **Repository Structure** for folders and content.

    ``` bash
    resume-processing-pipeline/
    â”œâ”€â”€ architecture/                      # Architecture assets
    â”‚   â”œâ”€â”€ architecture_diagram.png       # Final architecture image
    â”œâ”€â”€ screenshots/                       # Demo screenshots of different components
    â”‚
    â”œâ”€â”€ lambda_upload_handler/            # Lambda function for resume upload
    â”‚   â”œâ”€â”€ Dockerfile                     # Dockerfile for Lambda packaging
    â”‚   â”œâ”€â”€ app.py                         # Lambda function entry point
    â”‚   â””â”€â”€ requirements.txt               # Python dependencies
    â”‚
    â”œâ”€â”€ lambda_extractor/                 # Lambda for parsing resumes and extracting data
    â”‚   â”œâ”€â”€ extractor.py
    â”‚   â””â”€â”€ requirements.txt
    â”‚
    â”œâ”€â”€ glue_jobs/                         # AWS Glue ETL scripts
    â”‚   â””â”€â”€ resume_transform_job.py
    â”‚
    â”œâ”€â”€ cloudformation_templates/         # Infrastructure as Code templates
    â”‚   â””â”€â”€ deployment_template.yaml
    â”‚
    â”œâ”€â”€ project_report/                   # Detailed project report
    â”‚   â””â”€â”€ Resume_Processing_Project_Report.pdf
    â”‚
    â”œâ”€â”€ config/                            # Configuration files
    â”‚   â””â”€â”€ glue_crawler_config.json
    â”œâ”€â”€ README.md                          # Project overview and setup instructions
    â”œâ”€â”€ LICENSE                            # Open-source license (MIT)
    â””â”€â”€ .gitignore                         # Git ignored files


---

## ğŸ§ª Sample Athena Query
- sql
    ```sql
    SELECT name, email, skills 
    FROM resume_data_transformed 
    WHERE ARRAY_CONTAINS(skills, 'AWS');

## ğŸ³ Resume Parser Lambda

Dockerized AWS Lambda handler for parsing complex PDF formats and NLP pre-processing.

## ğŸš€ Deployment

### Build & Push to ECR
- bash
    ```bash
    docker build -t resume-parser-lambda .

    aws ecr create-repository --repository-name resume-parser-lambda

    docker tag resume-parser-lambda:latest <account_id>.dkr.ecr.<region>.amazonaws.com/resume-parser-lambda

    docker push <account_id>.dkr.ecr.<region>.amazonaws.com/resume-parser-lambda

## Validation Steps

###  1. Use **Postman** or `curl`:
- bash
    ```bash
    curl -X POST https://<api_gateway_url>/upload \
    -F "file=@sample_resume.pdf"

### 2. Post-Upload Validation Checklist

| Step | Service         | âœ… Validation Criteria                                |
|------|------------------|-------------------------------------------------------|
| 1    | ğŸ“‚ **S3**         | PDF is saved in the **raw bucket**                   |
| 2    | ğŸ”„ **Lambda**     | CloudWatch **logs show event trigger**               |
| 3    | ğŸ—ƒï¸ **DynamoDB**   | Parsed resume data is **stored as a new entry**       |
| 4    | ğŸ§ª **Athena**     | Data is **queryable via SQL**                        |
| 5    | ğŸ“Š **QuickSight** | Dashboard **reflects the new data** update           |

    
## ğŸ“„ Project Report
See complete documentation in [`project_report/Resume_Processing_Project_Report.pdf`](project_report/Resume_Processing_Project_Report.pdf) containing:
- Problem statement
- Architecture design
- Component responsibilities
- Data flow explanation
- AWS service cost estimation (optional)
- Future enhancements

## ğŸ› ï¸ Setup & Deployment
Choose one method:

### CloudFormation

aws cloudformation deploy --template-file cloudformation_templates/deployment_template.yaml


### Manual Deployment
1. Deploy Docker Lambda to ECR
2. Create S3 buckets for input/output
3. Setup DynamoDB table for metadata
4. Configure IAM roles and permissions
5. Set up AWS Glue crawler and ETL jobs
6. Create Athena database and tables
7. Connect Athena to QuickSight for visualization

## ğŸ”® Future Improvements
- Implement ML-based resume ranking (job description matching)
- Add notification system (SNS/email) for processing completion
- Integrate OCR for scanned PDF resumes
- Enhance error handling and retry mechanisms

## ğŸ“œ License
**MIT License** - Open source project. See [LICENSE](LICENSE) for details.
